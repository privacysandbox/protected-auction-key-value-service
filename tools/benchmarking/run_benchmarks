#!/bin/bash
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -o pipefail
set -o errexit

START=$(date +%s)
readonly START

WORKSPACE="$(git rev-parse --show-toplevel)"

BASE_OUTPUT_DIR="${WORKSPACE}/dist/tools/benchmarking/output/${START}"

NUMBER_OF_LOOKUP_KEYS="1 5 10"
REQUEST_METADATA="{}"
FILTER_BY_SETS=0
ENCRYPT_REQUESTS=0
FILTER_BY_SETS_JSON='{"filter_snapshot_by_sets": "false"}'
BENCHMARK_DURATION="5s"

DOCKER_OUTPUT_DIR="/tmp/benchmarking/output"
readonly DOCKER_OUTPUT_DIR

DOCKER_SNAPSHOT_DIR="/tmp/benchmarking/snapshots"
DOCKER_SNAPSHOT_CSV_DIR="/tmp/benchmarking/snapshot_csvs"
DOCKER_LOOKUP_KEYS_DIR="/tmp/benchmarking/lookup_keys/"

QUERY_ENCRYPTOR="//tools/benchmarking:query_encryptor"
QUERY_ENCRYPTOR_BINARY_RELPATH="bazel-bin/tools/benchmarking/query_encryptor"
QUERY_ENCRYPTOR_PATH="${WORKSPACE}/${QUERY_ENCRYPTOR_BINARY_RELPATH}"

function usage() {
  local -r -i exitval=${1-1}
  cat &>/dev/stderr <<USAGE
usage:
  ${BASH_SOURCE[0]} <flags>
    [--server-address]                (Required) gRPC host and port.
    [--snapshot-dir] OR               (Required) Full path to either snapshot-dir, snapshot-csv-dir,
    [--snapshot-csv-dir] OR            or lookup-keys-file is required.
    [--lookup-keys-file]               The lookup-keys-file should have one lookup key per line
                                       and ignores the filter-snapshot-by-sets option.
    [--number-of-lookup-keys-list]    (Optional) List of number of keys to include in a request.
    [--benchmark-duration]            (Optional) Duration of each benchmark. Default "5s".
    [--ghz-tags]                      (Optional) Tags to include in the ghz run.
    [--request-metadata-json]         (Optional) Request metadata to set for all requests
    [--filter-snapshot-by-sets]       (Optional) Whether to filter snapshots by sets when creating requests.
    [--encrypt-requests]              (Optional) Whether to encrypt the requests with hardcoded keys.
USAGE
  # shellcheck disable=SC2086
  exit ${exitval}
}

function generate_requests_with_file() {
  # Create output dirs before calling bazel-debian
  # If the dir is created in the docker container, we lose write permission
  LOOKUP_KEYS_FILENAME=$(basename "${LOOKUP_KEYS_FILE}")
  LOOKUP_KEYS_FILE_DIR=$(dirname "${LOOKUP_KEYS_FILE}")
  for N in "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}"; do
    mkdir -p "${BASE_OUTPUT_DIR}/${LOOKUP_KEYS_FILENAME}/n=${N}"
  done

  local -a GENERATE_REQUESTS_ARGS=(
    --output-dir "${DOCKER_OUTPUT_DIR}"
    --number-of-keys-list "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}"
    --metadata "${REQUEST_METADATA}"
    --lookup-keys-file "${DOCKER_LOOKUP_KEYS_DIR}/${LOOKUP_KEYS_FILENAME}"
  )
  if [[ ${FILTER_BY_SETS} -eq 1 ]]; then
    GENERATE_REQUESTS_ARGS+=(--filter-by-sets)
    FILTER_BY_SETS_JSON='{"filter_snapshot_by_sets": "true"}'
  fi
  if [[ ${ENCRYPT_REQUESTS} -eq 0 ]]; then
    GENERATE_REQUESTS_ARGS+=(--encode-requests)
  fi

  # Mount the output dir to docker and write requests to output dir for each item in
  # `NUMBER_OF_LOOKUP_KEYS_LIST`.
  # This will write a json request for each NUMBER_OF_LOOKUP_KEY=N to
  # dist/tools/benchmarking/output/${START}/${LOOKUP_KEYS_FILENAME}/n=${N}/request.json
  EXTRA_DOCKER_RUN_ARGS+=" --volume ${BASE_OUTPUT_DIR}:${DOCKER_OUTPUT_DIR} --volume ${LOOKUP_KEYS_FILE_DIR}:${DOCKER_LOOKUP_KEYS_DIR} " \
    builders/tools/bazel-debian run //tools/benchmarking:generate_requests \
    -- "${GENERATE_REQUESTS_ARGS[@]}"
}

function generate_requests_with_snapshots() {
  # Create output dirs before calling bazel-debian
  # If the dir is created in the docker container, we lose write permission
  for SNAPSHOT_CSV in "${SNAPSHOT_CSV_DIR}"/*; do
    SNAPSHOT_CSV_FILENAME=$(basename "${SNAPSHOT_CSV}")
    for N in "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}"; do
      mkdir -p "${BASE_OUTPUT_DIR}/${SNAPSHOT_CSV_FILENAME}/n=${N}"
    done
  done

  local -a GENERATE_REQUESTS_ARGS
  GENERATE_REQUESTS_ARGS+=(--output-dir "${DOCKER_OUTPUT_DIR}")
  GENERATE_REQUESTS_ARGS+=(--number-of-keys-list "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}")
  GENERATE_REQUESTS_ARGS+=(--metadata "${REQUEST_METADATA}")
  GENERATE_REQUESTS_ARGS+=(--snapshot-csv-dir "${DOCKER_SNAPSHOT_CSV_DIR}")
  if [[ "${FILTER_BY_SETS}" = 1 ]]; then
    GENERATE_REQUESTS_ARGS+=(--filter-by-sets)
    FILTER_BY_SETS_JSON='{"filter_snapshot_by_sets": "true"}'
  fi
  if [[ ${ENCRYPT_REQUESTS} -eq 0 ]]; then
    GENERATE_REQUESTS_ARGS+=(--encode-requests)
  fi

  # Mount the output dir to docker and write requests to output dir for each item in
  # `NUMBER_OF_LOOKUP_KEYS_LIST`.
  # This will write a json request for each NUMBER_OF_LOOKUP_KEY=N to
  # dist/tools/benchmarking/output/${START}/${SNAPSHOT_FILENAME}/n=${N}/request.json
  EXTRA_DOCKER_RUN_ARGS+=" --volume ${BASE_OUTPUT_DIR}:${DOCKER_OUTPUT_DIR} --volume ${SNAPSHOT_CSV_DIR}:${DOCKER_SNAPSHOT_CSV_DIR} " \
    builders/tools/bazel-debian run //tools/benchmarking:generate_requests \
    -- "${GENERATE_REQUESTS_ARGS[@]}"
}

function run_ghz_for_requests_from_file() {
  FILENAME=$(basename "${1}")
  REQUEST_JSON_DOCKER_OUTPUT_DIR="${DOCKER_OUTPUT_DIR}/${FILENAME}"
  REQUEST_JSON_BASE_OUTPUT_DIR="${BASE_OUTPUT_DIR}/${FILENAME}"

  local V2_ENDPOINT="GetValuesHttp"
  local REQUEST_FILENAME="encoded_request.json"
  if [[ ${ENCRYPT_REQUESTS} -eq 1 ]]; then
      V2_ENDPOINT="ObliviousGetValues"
      REQUEST_FILENAME="encrypted_request.json"
  fi
  for N in "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}"; do
    DIR="${REQUEST_JSON_DOCKER_OUTPUT_DIR}/n=${N}"
    REQUEST_JSON_DOCKER="${DIR}"/"${REQUEST_FILENAME}"
    REQUEST_PLAIN_TEXT_JSON_BASE="${REQUEST_JSON_BASE_OUTPUT_DIR}/n=${N}"/plaintext_request.json
    REQUEST_JSON_BASE="${REQUEST_JSON_BASE_OUTPUT_DIR}/n=${N}"/"${REQUEST_FILENAME}"
    if [[ ${ENCRYPT_REQUESTS} -eq 1 ]]; then
      run_encryption_tool_host "${REQUEST_PLAIN_TEXT_JSON_BASE}" "${REQUEST_JSON_BASE}"
    fi
    if [[ ! -f "${REQUEST_JSON_BASE}" ]]; then
      continue
    fi

    printf "Running ghz for number of keys %s\n" "${N}"
    BASE_GHZ_TAGS=$(
      builders/tools/jq -n --arg n "${N}" --arg f "${FILENAME}" \
        '{"number_of_lookup_keys": $n, "keys_from_file": $f}'
    )
    REQUEST_METADATA_TAGS=$(echo "${REQUEST_METADATA}" | builders/tools/jq 'with_entries(.key |= "request_metadata."+.) | with_entries( .value |= @json)')
    TAGS=$(echo "${GHZ_TAGS} ${BASE_GHZ_TAGS} ${REQUEST_METADATA_TAGS} ${FILTER_BY_SETS_JSON}" | builders/tools/jq -s -c 'add')
    GHZ_OUTPUT_JSON_FILE="${DIR}/ghz_output.json"
    EXTRA_DOCKER_RUN_ARGS+=" --volume ${BASE_OUTPUT_DIR}:${DOCKER_OUTPUT_DIR} --volume ${WORKSPACE}:/src/workspace" \
      builders/tools/ghz --protoset /src/workspace/dist/query_api_descriptor_set.pb \
      -D "${REQUEST_JSON_DOCKER}" \
      --duration="${BENCHMARK_DURATION}" \
      --skipFirst=100 \
      --concurrency=100 \
      --format=pretty \
      --tags "${TAGS}" \
      --output "${GHZ_OUTPUT_JSON_FILE}" \
      --call kv_server.v2.KeyValueService/${V2_ENDPOINT} \
      "${SERVER_ADDRESS}"
    # In case the server hangs from previous requests, wait before sending next batch
    sleep 30
  done
}

function build_encryption_tool() {
  builders/tools/bazel-debian build \
      --config=local_instance \
      --config=gcp_platform \
      --config=nonprod_mode \
      "${QUERY_ENCRYPTOR}"
  build_status=$?
  if [[ ${build_status} -ne 0 ]]; then
      echo "ERROR: Failed to build (${QUERY_ENCRYPTOR}). Exiting." >&2
      exit 1
  fi
  # Verify the expected output binary exists
  if [[ ! -x "${QUERY_ENCRYPTOR_PATH}" ]]; then
      echo "ERROR: Build succeeded but encryptor binary not found or not executable at expected path: ${QUERY_ENCRYPTOR_PATH}" >&2
      exit 1
  fi
  echo "query encryptor built successfully: ${QUERY_ENCRYPTOR_PATH}"
}

function run_encryption_tool_host() {
  local -r plaintext_request_path_host="${1}"
  local -r final_request_path_host="${2}"

  echo "Encrypting request: ${plaintext_request_path_host} -> ${final_request_path_host}"

  local -a ENCRYPT_ARGS=(
    "--input_json_path=${plaintext_request_path_host}"
    "--output_json_path=${final_request_path_host}"
    "--stderrthreshold=0"
  )

  if [[ ! -x "${QUERY_ENCRYPTOR_PATH}" ]]; then
      echo "ERROR: Encryptor binary not found or not executable at ${QUERY_ENCRYPTOR_PATH}" >&2
      return 1
  fi

  "${QUERY_ENCRYPTOR_PATH}" "${ENCRYPT_ARGS[@]}"
  local exit_code=$? # Capture exit code immediately

  if [[ ${exit_code} -ne 0 ]]; then
      echo "ERROR: Encryption tool execution failed with exit code ${exit_code}." >&2
      return 1
  fi

  if [[ ! -f "${final_request_path_host}" ]]; then
     echo "ERROR: Encryption command succeeded but output file ${final_request_path_host} not found." >&2
     return 1
  fi

   echo "Encryption successful."
   return 0
}

function run_ghz_for_requests() {
  if [[ ${ENCRYPT_REQUESTS} -eq 1 ]]; then
    build_encryption_tool
  fi
  # Iterate through the generated request.json files and
  # call `ghz` to benchmark server at ${SERVER_ADDRESS}
  if [[ -f "${LOOKUP_KEYS_FILE}" ]]; then
    run_ghz_for_requests_from_file "${LOOKUP_KEYS_FILE}"
  else
    for SNAPSHOT_CSV_FILE in "${SNAPSHOT_CSV_DIR}"/*; do
      run_ghz_for_requests_from_file "${SNAPSHOT_CSV_FILE}"
    done
  fi
}

while [[ $# -gt 0 ]]; do
  case "$1" in
  --server-address)
    SERVER_ADDRESS="$2"
    shift 2 || usage
    ;;
  --number-of-lookup-keys-list)
    NUMBER_OF_LOOKUP_KEYS="$2"
    shift 2
    ;;
  --benchmark-duration)
    BENCHMARK_DURATION="$2"
    shift 2
    ;;
  --ghz-tags)
    GHZ_TAGS="$2"
    shift 2
    ;;
  --snapshot-dir)
    SNAPSHOT_DIR="$2"
    shift 2
    ;;
  --snapshot-csv-dir)
    SNAPSHOT_CSV_DIR="$2"
    shift 2
    ;;
  --lookup-keys-file)
    LOOKUP_KEYS_FILE="$2"
    shift 2
    ;;
  --request-metadata-json)
    REQUEST_METADATA="$2"
    shift 2
    ;;
  --filter-snapshot-by-sets)
    FILTER_BY_SETS=1
    shift
    ;;
  --encrypt-requests)
    ENCRYPT_REQUESTS=1
    shift
    ;;
  -h | --help) usage 0 ;;
  *) usage ;;
  esac
done

# Check for SNAPSHOT_DIR or SNAPSHOT_CSV_DIR or LOOKUP_KEYS_FILE. If not available, exit.
if [[ (-z "${SNAPSHOT_DIR}" || ! -d "${SNAPSHOT_DIR}") && (-z "${SNAPSHOT_CSV_DIR}" || ! -d "${SNAPSHOT_CSV_DIR}") && (-z "${LOOKUP_KEYS_FILE}" || ! -f "${LOOKUP_KEYS_FILE}") ]]; then
  printf "snapshot-dir not found:%s\n" "${SNAPSHOT_DIR}"
  printf "snapshot-csv-dir not found:%s\n" "${SNAPSHOT_CSV_DIR}"
  printf "lookup-keys-file not found:%s\n" "${LOOKUP_KEYS_FILE}"
  printf "Exiting...\n"
  exit 1
fi

IFS=' ' read -ra NUMBER_OF_LOOKUP_KEYS_LIST <<<"${NUMBER_OF_LOOKUP_KEYS}"
if [[ -f ${LOOKUP_KEYS_FILE} ]]; then
  generate_requests_with_file
elif [[ -d ${SNAPSHOT_CSV_DIR} ]]; then
  generate_requests_with_snapshots
else
  # No snapshot csv given, iterate through snapshot dir to create csvs
  SNAPSHOT_CSV_DIR="${BASE_OUTPUT_DIR}/snapshot_csvs/"
  # Iterate through snapshot files and convert them to CSV
  mkdir -p "${SNAPSHOT_CSV_DIR}"
  for SNAPSHOT_FILE in "${SNAPSHOT_DIR}"/*; do
    SNAPSHOT_FILENAME=$(basename "${SNAPSHOT_FILE}")
    EXTRA_DOCKER_RUN_ARGS+=" --volume ${SNAPSHOT_CSV_DIR}:${DOCKER_SNAPSHOT_CSV_DIR} --volume ${SNAPSHOT_DIR}:${DOCKER_SNAPSHOT_DIR} " \
      builders/tools/bazel-debian run //tools/data_cli:data_cli format_data \
      -- \
      --input_file "${DOCKER_SNAPSHOT_DIR}/${SNAPSHOT_FILENAME}" \
      --input_format DELTA \
      --output_file "${DOCKER_SNAPSHOT_CSV_DIR}/${SNAPSHOT_FILENAME}.csv" \
      --output_format CSV
  done
  generate_requests_with_snapshots
fi

run_ghz_for_requests

# Go through all the ghz results in dist/tools/benchmarking/output/${START}
# and collect the summary in a csv
EXTRA_DOCKER_RUN_ARGS+=" --volume ${BASE_OUTPUT_DIR}:${DOCKER_OUTPUT_DIR} " \
  builders/tools/bazel-debian run //tools/benchmarking:create_csv_summary \
  -- \
  --ghz-result-dir ${DOCKER_OUTPUT_DIR}

echo "Result in:"
echo "${BASE_OUTPUT_DIR}/summary.csv"
