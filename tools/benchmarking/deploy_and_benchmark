#!/bin/bash
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -o pipefail
set -o errexit

# shellcheck disable=SC1090
source ./builders/tools/builder.sh

START=$(date +%s)
WORKSPACE="$(git rev-parse --show-toplevel)"

# Script input params
EXTRA_SERVER_WAIT_TIMEOUT="5m"
MINIMUM_SERVER_WAIT_SECS=90
NUMBER_OF_LOOKUP_KEYS_LIST="1 10 50 100"
GHZ_TAGS="{}"
FILTER_BY_SETS=0

# Additional GHZ tags per deployment/udf
DEPLOYMENT_GHZ_TAGS="{}"
UDF_GHZ_TAGS="{}"

# Input to run_benchmark script
declare RUN_BENCHMARK_ARGS
BENCHMARK_DURATION="5s"

# Deployment vars
declare SERVER_ADDRESS
declare SERVER_ENDPOINT

# CSV I/O
declare -a BENCHMARK_CSVS
CSV_OUTPUT="${WORKSPACE}/dist/tools/benchmarking/output/output.csv"
declare -r DOCKER_OUTPUT_CSV="/tmp/benchmarking/output/deploy_and_benchmark/output.csv"
declare -r SNAPSHOT_CSV_DIR="${WORKSPACE}/dist/tools/benchmarking/deploy_and_benchmark/snapshot_csvs/${START}"
declare -r DOCKER_SNAPSHOT_CSV_DIR="/tmp/benchmarking/deploy_and_benchmark/snapshot_csvs/"
declare -r DOCKER_SNAPSHOT_DIR="/tmp/benchmarking/deploy_and_benchmark/snapshots/"

# run_benchmarks writes output to this directory
CSV_SUMMARY_INPUT_DIR="${WORKSPACE}/dist/tools/benchmarking/output"
DOCKER_INPUT_DIR="/tmp/benchmarking/summaries/"
readonly DOCKER_INPUT_DIR

DESTROY_INSTANCES=0
TF_DEPLOY_SUCCESS=0

trap _destroy EXIT
function _destroy() {
  if [[ ${DESTROY_INSTANCES} == 1 && ${TF_DEPLOY_SUCCESS} == 1 ]]; then
    printf "Running terraform destroy\n"
    builders/tools/terraform \
      -chdir="${WORKSPACE}"/production/terraform/"${CLOUD_PROVIDER}"/environments \
      destroy --var-file="${TF_VAR_FILE}" --auto-approve >/dev/null
  fi
}

trap _trap ERR
function _trap() {
  local -r -i STATUS=$?
  FAILED_COMMAND="${BASH_COMMAND}"
  printf "Failed command: %s\n" "${FAILED_COMMAND}"
  exit ${STATUS}
}

function usage() {
  local -r -i exitval=${1-1}
  cat &>/dev/stderr <<USAGE
usage:
  ${BASH_SOURCE[0]} <flags>
  [--cloud-provider]                  (Required) Cloud provider. Options: "aws" or "gcp"
  [--tf-var-file]                     (Required) Full path to tfvars.json file.
  [--tf-backend-config]               (Required) Full path to tf backend.conf file.
  [--server-url]                      (Required) URL of deployed server.
  [--snapshot-dir] OR                 (Required) Full path to a directory of snapshot files
  [--lookup-keys-file]                 or a file with lookup keys.
                                       The lookup-keys-file should have one lookup key per line
                                       and ignores the filter-snapshot-by-sets option.
  [--csv-output]                      (Optional) Path to output file for summary of benchmarks.
  [--tf-overrides]                    (Optional) Path to file with terraform variable overrides.
  [--udf-delta-dir]                   (Optional) Full path to directory of udf delta files.
  [--data-bucket]                     (Optional) Data bucket to upload the udf files too.
  [--request-metadata-json-file]      (Optional) Path to JSON lines file with request metadata.
  [--number-of-lookup-keys-list]      (Optional) List of number of keys to include in a request.
  [--minimum-server-wait-secs]        (Optional) Amount of time to wait before checking server health.
  [--extra-server-wait-timeout]       (Optional) Timeout for waiting for server to become healthy.
  [--benchmark-duration]              (Optional) Duration of each benchmark. Default "5s".
  [--ghz-tags]                        (Optional) Tags to include in the ghz run.
  [--cleanup-deployment]              (Optional) Whether to call terraform destroy once the tool exits.
  [--filter-snapshot-by-sets]         (Optional) Whether to filter snapshots by sets when creating requests.
  [--encrypt-requests]                (Optional) Whether to encrypt the requests with hardcoded keys.
USAGE
  # shellcheck disable=SC2086
  exit ${exitval}
}

function convert_snapshots_to_csvs() {
  mkdir -p "${SNAPSHOT_CSV_DIR}"
  # Iterate through snapshot files and convert them to CSV
  for SNAPSHOT_FILE in "${SNAPSHOT_DIR}"/*; do
    SNAPSHOT_FILENAME=$(basename "${SNAPSHOT_FILE}")
    EXTRA_DOCKER_RUN_ARGS+=" --volume ${SNAPSHOT_CSV_DIR}:${DOCKER_SNAPSHOT_CSV_DIR} --volume ${SNAPSHOT_DIR}:${DOCKER_SNAPSHOT_DIR} " \
      builders/tools/bazel-debian run //tools/data_cli:data_cli format_data \
      -- \
      --input_file "${DOCKER_SNAPSHOT_DIR}/${SNAPSHOT_FILENAME}" \
      --input_format DELTA \
      --output_file "${DOCKER_SNAPSHOT_CSV_DIR}/${SNAPSHOT_FILENAME}.csv" \
      --output_format CSV
  done
}

function set_benchmark_args() {
  local -a RUN_BENCHMARK_GHZ_TAGS
  RUN_BENCHMARK_GHZ_TAGS=$(jq -s -c 'add' <(echo "${GHZ_TAGS}") \
    <(echo "${DEPLOYMENT_GHZ_TAGS}") \
    <(echo "${UDF_GHZ_TAGS}"))
  RUN_BENCHMARK_ARGS=(
    --number-of-lookup-keys-list "${NUMBER_OF_LOOKUP_KEYS_LIST[@]}"
    --server-address "${SERVER_ADDRESS}"
    --ghz-tags "${RUN_BENCHMARK_GHZ_TAGS}"
    --benchmark-duration "${BENCHMARK_DURATION}"
  )
  if [[ -f "${LOOKUP_KEYS_FILE}" ]]; then
    RUN_BENCHMARK_ARGS+=(
      --lookup-keys-file "${LOOKUP_KEYS_FILE}"
    )
  else
    RUN_BENCHMARK_ARGS+=(
      --snapshot-csv-dir "${SNAPSHOT_CSV_DIR}"
    )
  fi
  if [[ -v "${REQUEST_METADATA_JSON}" ]]; then
    RUN_BENCHMARK_ARGS+=(
      --request-metadata-json "${REQUEST_METADATA_JSON}"
    )
  fi
  if [[ "${FILTER_BY_SETS}" = 1 ]]; then
    RUN_BENCHMARK_ARGS+=(
      --filter-snapshot-by-sets
    )
  fi
  if [[ "${ENCRYPT_REQUESTS}" = 1 ]]; then
    RUN_BENCHMARK_ARGS+=(
      --encrypt-requests
    )
  fi
}

function run_benchmarks() {
  if [[ -z "${REQUEST_METADATA_JSON_FILE}" ]]; then
    set_benchmark_args
    printf "BENCHMARK ARGS: %s\n" "${RUN_BENCHMARK_ARGS[*]}"

    local BENCHMARK_OUTPUT
    BENCHMARK_OUTPUT=$(./tools/benchmarking/run_benchmarks "${RUN_BENCHMARK_ARGS[@]}")
    BENCHMARK_CSVS+=(
      "$(echo "${BENCHMARK_OUTPUT}" | tail -n 1 2>&1 | tee /dev/tty)"
    )
  else
    while IFS= read -r REQUEST_METADATA_JSON; do
      set_benchmark_args
      printf "BENCHMARK ARGS: %s\n" "${RUN_BENCHMARK_ARGS[*]}"

      local BENCHMARK_OUTPUT
      BENCHMARK_OUTPUT=$(./tools/benchmarking/run_benchmarks "${RUN_BENCHMARK_ARGS[@]}")
      BENCHMARK_CSVS+=(
        "$(echo "${BENCHMARK_OUTPUT}" | tail -n 1 2>&1 | tee /dev/tty)"
      )
    done <"${REQUEST_METADATA_JSON_FILE}"
  fi
}

function set_server_address() {
  # Build HTTP server endpoint from tf output
  local SERVER_HOSTNAME
  SERVER_ENDPOINT="${SERVER_URL}/v1/getvalues?keys=hi"
  # Build gRPC server address from tf output
  SERVER_HOSTNAME=$([[ "${SERVER_URL}" =~ https://(.*) ]] && echo "${BASH_REMATCH[1]}")
  if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
    SERVER_ADDRESS="${SERVER_HOSTNAME}:8443"
  elif [[ "${CLOUD_PROVIDER}" == "gcp" ]]; then
    SERVER_ADDRESS="${SERVER_HOSTNAME}:443"
  else
    echo "Cloud provider not supported"
    exit 1
  fi
}

function upload_file_to_bucket() {
  if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
    EXTRA_DOCKER_RUN_ARGS+=" --volume ${1}:/tmp/deltas/${1}" \
      builders/tools/aws-cli s3 cp "/tmp/deltas/${1}" "${2}"
  elif [[ "${CLOUD_PROVIDER}" == "gcp" ]]; then
    gcloud storage cp "${1}" "${2}"
  else
    echo "Cloud provider not supported"
    exit 1
  fi
}

function remove_file_from_bucket() {
  if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
    builders/tools/aws-cli s3 rm "${1}"
  elif [[ "${CLOUD_PROVIDER}" == "gcp" ]]; then
    gcloud storage rm "${1}"
  else
    echo "Cloud provider not supported"
    exit 1
  fi
}

function deploy_and_benchmark() {
  printf "Running terraform init\n"
  builders/tools/terraform \
    -chdir=production/terraform/"${CLOUD_PROVIDER}"/environments \
    init --backend-config="${TF_BACKEND_CONFIG}" \
    --var-file="${TF_VAR_FILE}" --reconfigure -input=false \
    >/dev/null

  printf "Running terraform apply with var file: %s\n" "${TF_VAR_FILE}"
  printf "and var overrides: %s\n" "${VAR_OVERRIDES[*]}"
  builders/tools/terraform \
    -chdir=production/terraform/"${CLOUD_PROVIDER}"/environments \
    apply --var-file="${TF_VAR_FILE}" "${VAR_OVERRIDES[@]}" \
    -auto-approve
  TF_DEPLOY_SUCCESS=1
  printf "Done applying terraform, waiting for server to be ready\n"

  set_server_address

  # Wait for potential instance teardown before periodically checking if server is ready
  sleep "${MINIMUM_SERVER_WAIT_SECS}"

  # Wait for server to be up
  timeout --foreground "${EXTRA_SERVER_WAIT_TIMEOUT}" bash -c \
    "until curl --output /dev/null --silent --fail ${SERVER_ENDPOINT};do sleep 15; done"
  printf "Server ready\n"

  # Iterate through each given UDF
  # Upload delta file, run benchmarks, then remove it
  if [[ -d "${UDF_DELTA_DIR}" ]]; then
    for FILE in "${UDF_DELTA_DIR}"/*; do
      local FILENAME
      FILENAME=$(basename "${FILE}")
      printf "Uploading UDF file %s to cloud storage %s\n" "${FILE}" "${DATA_BUCKET}"
      upload_file_to_bucket "${FILE}" "${DATA_BUCKET}/${FILENAME}"
      # Allow some time for server to pick up new UDF
      sleep 60
      UDF_GHZ_TAGS="{ \"udf_delta_file\": \"${FILENAME}\" }"
      run_benchmarks
      printf "Removing file from cloud storage %s\n" "${DATA_BUCKET}/${FILENAME}"
      remove_file_from_bucket "${DATA_BUCKET}/${FILENAME}"
    done
  else
    run_benchmarks
  fi
}

function merge_benchmark_csvs() {
  # Map csv summary files from run_benchmarks to docker volume paths
  # so that we can access them from within builders/tools/bazel-debian
  # i.e. csv_output_dir/summary.csv -> docker_mounted_dir/summary.csv
  declare -a DOCKER_BENCHMARK_CSVS
  for BENCHMARK_CSV in "${BENCHMARK_CSVS[@]}"; do
    DOCKER_BENCHMARK_CSVS+=(
      "${DOCKER_INPUT_DIR}${BENCHMARK_CSV#"${CSV_SUMMARY_INPUT_DIR}"}"
    )
  done
  touch "${CSV_OUTPUT}"
  # Run merge_csvs python script
  EXTRA_DOCKER_RUN_ARGS+=" --volume ${CSV_SUMMARY_INPUT_DIR}:${DOCKER_INPUT_DIR} --volume ${CSV_OUTPUT}:${DOCKER_OUTPUT_CSV} " \
    builders/tools/bazel-debian run //tools/benchmarking:merge_csvs \
    -- \
    --csv-inputs "${DOCKER_BENCHMARK_CSVS[@]}" \
    --csv-output "${DOCKER_OUTPUT_CSV}"

  printf "Results in: %s\n" "${CSV_OUTPUT}"
}

while [[ $# -gt 0 ]]; do
  case "$1" in
  --tf-var-file)
    TF_VAR_FILE="$2"
    shift 2 || usage
    ;;
  --tf-backend-config)
    TF_BACKEND_CONFIG="$2"
    shift 2 || usage
    ;;
  --tf-overrides)
    TF_OVERRIDES="$2"
    shift 2
    ;;
  --csv-output)
    CSV_OUTPUT="$2"
    shift 2
    ;;
  --snapshot-dir)
    SNAPSHOT_DIR="$2"
    shift 2
    ;;
  --lookup-keys-file)
    LOOKUP_KEYS_FILE="$2"
    shift 2
    ;;
  --udf-delta-dir)
    UDF_DELTA_DIR="$2"
    shift 2
    ;;
  --data-bucket)
    DATA_BUCKET="$2"
    shift 2
    ;;
  --extra-server-wait-timeout)
    EXTRA_SERVER_WAIT_TIMEOUT="$2"
    shift 2
    ;;
  --number-of-lookup-keys-list)
    NUMBER_OF_LOOKUP_KEYS_LIST="$2"
    shift 2
    ;;
  --benchmark-duration)
    BENCHMARK_DURATION="$2"
    shift 2
    ;;
  --ghz-tags)
    GHZ_TAGS="$2"
    shift 2
    ;;
  --minimum-server-wait-secs)
    MINIMUM_SERVER_WAIT_SECS="$2"
    shift 2
    ;;
  --cloud-provider)
    CLOUD_PROVIDER="$2"
    shift 2
    ;;
  --server-url)
    SERVER_URL="$2"
    shift 2
    ;;
  --cleanup-deployment)
    DESTROY_INSTANCES=1
    shift
    ;;
  --request-metadata-json-file)
    REQUEST_METADATA_JSON_FILE="$2"
    shift 2
    ;;
  --filter-snapshot-by-sets)
    FILTER_BY_SETS=1
    shift
    ;;
  --encrypt-requests)
    ENCRYPT_REQUESTS=1
    shift
    ;;
  -h | --help) usage 0 ;;
  *) usage ;;
  esac
done

if [[ -z "${CLOUD_PROVIDER}" || ! ("${CLOUD_PROVIDER}" == "aws" || "${CLOUD_PROVIDER}" == "gcp") ]]; then
  printf "cloud-provider must equal \"aws\" or \"gcp\""
  exit 1
fi

if [[ -z "${SERVER_URL}" ]]; then
  printf "server-url must be provided"
  exit 1
fi

# Check for SNAPSHOT_DIR or LOOKUP_KEYS_FILE. If not available, exit.
if [[ (-z "${SNAPSHOT_DIR}" || ! -d "${SNAPSHOT_DIR}") && (-z "${LOOKUP_KEYS_FILE}" || ! -f "${LOOKUP_KEYS_FILE}") ]]; then
  printf "snapshot-dir not found:%s\n" "${SNAPSHOT_DIR}"
  printf "lookup-keys-file not found:%s\n" "${LOOKUP_KEYS_FILE}"
  exit 1
fi

# Check that files, if provided, are readable
if [[ -v "${TF_OVERRIDES}" && ! -r "${TF_OVERRIDES}" ]]; then
  printf "tf-overrides file not readable: %s\n" "${TF_OVERRIDES}"
  exit 1
fi

if [[ -v "${REQUEST_METADATA_JSON_FILE}" && ! -r "${REQUEST_METADATA_JSON_FILE}" ]]; then
  printf "request-metadata-json-file file not readable: %s\n" "${REQUEST_METADATA_JSON_FILE}"
  exit 1
fi

if [[ -d "${SNAPSHOT_DIR}" ]]; then
  convert_snapshots_to_csvs
fi

# No terraform variable overrides, deploy and benchmark without overrides
if [[ -z "${TF_OVERRIDES}" ]]; then
  deploy_and_benchmark
else
  # Terraform override file found:
  # Each row defines a set of overrides for terraform variables.
  # Pass the overrides to deploy_and_benchmark function and set
  # them as tags in the ghz call.
  TF_OVERRIDES_LIST=()
  while IFS=',' read -ra VARS; do
    TF_OVERRIDES_LIST+=("${VARS[*]}")
  done <"${TF_OVERRIDES}"

  for TF_OVERRIDES in "${TF_OVERRIDES_LIST[@]}"; do
    IFS=" " read -ra VAR_LIST <<<"${TF_OVERRIDES}"
    declare -a VAR_OVERRIDES=()
    DEPLOYMENT_GHZ_TAGS="{}"
    for VAR in "${VAR_LIST[@]}"; do
      VAR_OVERRIDES+=(-var "${VAR}")
      OVERRIDE_VAR_GHZ_TAG=$(echo "${VAR}" | jq -s -R 'split("\n") | .[0] | split("=") | {(.[0]): .[1]}')
      DEPLOYMENT_GHZ_TAGS=$(echo "${DEPLOYMENT_GHZ_TAGS} ${OVERRIDE_VAR_GHZ_TAG}" | jq -s -c 'add')
    done
    deploy_and_benchmark
  done
fi

# Benchmarks done, merge CSVs
merge_benchmark_csvs
